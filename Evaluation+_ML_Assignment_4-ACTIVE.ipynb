{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fwi4m_tQr1xs",
    "outputId": "9eafdd6e-ecd4-41c5-d7de-e31f3d8fe546"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1Pjr69ChG81L2KEQLPs-dA6eW4RUCkBsD\n",
      "To: C:\\Faaiq\\Course Resources\\Semester 4\\Machine Learning\\Assignment 4\\PA4_dataset.zip\n",
      "\n",
      "  0%|          | 0.00/322k [00:00<?, ?B/s]\n",
      "100%|##########| 322k/322k [00:00<00:00, 583kB/s]\n",
      "100%|##########| 322k/322k [00:00<00:00, 583kB/s]\n"
     ]
    }
   ],
   "source": [
    "!gdown --id 1Pjr69ChG81L2KEQLPs-dA6eW4RUCkBsD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZqsE9MlNsVqv",
    "outputId": "9751f7a2-9cd4-4505-fc33-e9f0e502a63b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'unzip' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# !unzip PA4_dataset.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f8cSqgtFs9xH"
   },
   "source": [
    "Loaded dataset, pre-processing now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "MHOWDRdutDCx"
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "TNvEDdByCbuq"
   },
   "outputs": [],
   "source": [
    "dataset_test = pd.read_csv(\"PA4_dataset/test.csv\")\n",
    "dataset_train = pd.read_csv(\"PA4_dataset/train.csv\")\n",
    "stop_words_doc = pd.read_csv(\"PA4_dataset/stop_words.txt\", header=None)\n",
    "stop_words = stop_words_doc[0][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9eeU-WIcEqbz"
   },
   "outputs": [],
   "source": [
    "# separating the sentiment data and the tweet content\n",
    "test_sentiment = dataset_test.iloc[:,0]\n",
    "train_sentiment = dataset_train.iloc[:,0]\n",
    "test_tweets = dataset_test.iloc[:,1]\n",
    "train_tweets = dataset_train.iloc[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LSSdU9x3h3uq"
   },
   "source": [
    "# Part 1: Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "H9hIO1hzMzJA"
   },
   "outputs": [],
   "source": [
    "# Beginning preprocessing\n",
    "# modulating by making function instead\n",
    "def cleaning_data(dataframe, stopwords):\n",
    "    clean_list = []\n",
    "    for tweet in dataframe:\n",
    "        temp_string = tweet\n",
    "        # removing stop words\n",
    "        for word in stopwords:\n",
    "            word = \" \"+word\n",
    "            temp_string = re.sub(word,'',temp_string) \n",
    "        # remove hyperlinks\n",
    "        temp_string = re.sub('http[s]?://\\S+', '', tweet)\n",
    "        # remove numbers\n",
    "        temp_string = re.sub(r'[0-9]+', '', temp_string)\n",
    "        # remove usernames\n",
    "        temp_string = re.sub('@\\S+','',temp_string)\n",
    "        # remove all punctuation\n",
    "        temp_string = re.sub(r'[^\\w\\s]', '', temp_string)\n",
    "        # done removing, add back to original list\n",
    "        temp_string = temp_string.lower()\n",
    "        clean_list.append(temp_string)\n",
    "    return clean_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for Naive Bayes\n",
    "# it is common to not remove stop words, hence we we will make a function that does everything, but does not remove stop words\n",
    "def cleaning_data_wsw(dataframe): # with stop words\n",
    "    clean_list = []\n",
    "    for tweet in dataframe:\n",
    "        temp_string = tweet\n",
    "        # remove hyperlinks\n",
    "        temp_string = re.sub('http[s]?://\\S+', '', tweet)\n",
    "        # remove numbers\n",
    "        temp_string = re.sub(r'[0-9]+', '', temp_string)\n",
    "        # remove usernames\n",
    "        temp_string = re.sub('@\\S+','',temp_string)\n",
    "        # remove all punctuation\n",
    "        temp_string = re.sub(r'[^\\w\\s]', '', temp_string)\n",
    "        # done removing, add back to original list\n",
    "        temp_string = temp_string.lower()\n",
    "        clean_list.append(temp_string)\n",
    "    return clean_list\n",
    "\n",
    "clean_sw_train = cleaning_data_wsw(train_tweets)\n",
    "clean_sw_test = cleaning_data_wsw(test_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r1a8Cs1aRDOY",
    "outputId": "a8caa39f-c20e-4596-9d50-ad9aebbde563"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       @USAirways thanks to the whole team for an on-...\n",
      "1       @VirginAmerica gives positive outlook, but see...\n",
      "2       @JetBlue Hey Jetblue, you Cancelled Flightled ...\n",
      "3       @united @PGATOUR @NTrustOpen Next thing you kn...\n",
      "4       @SouthwestAir why can't you help me after you ...\n",
      "                              ...                        \n",
      "1459    @AmericanAir after waiting for a delayed plane...\n",
      "1460    @VirginAmerica what is going on with customer ...\n",
      "1461    @SouthwestAir I will tell marry a lamp if you ...\n",
      "1462    @USAirways I got an email asking me to checkin...\n",
      "1463        @USAirways @AmericanAir no, don't leave me!!!\n",
      "Name: Tweet, Length: 1464, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "# pd.reset_option(\"all\")\n",
    "print(test_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "dDW5-dvd0g3G"
   },
   "outputs": [],
   "source": [
    "clean_test_tweets = cleaning_data(test_tweets, stop_words)\n",
    "clean_train_tweets = cleaning_data(train_tweets, stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CfWU1amocqel"
   },
   "source": [
    "Data Preprocessing done.\n",
    "Removed the following in order: \n",
    "\n",
    "\n",
    "*   Stop words first, as they contained punctuation.\n",
    "*   All usernames (everything starting with @)\n",
    "*   Punctuation\n",
    "*   Lowercase\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fgZZBcU5hhMo"
   },
   "source": [
    "# Part 2: Bag of Words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "8opAJ_5jhal1"
   },
   "outputs": [],
   "source": [
    "# Construct a vocabulary of all the words first\n",
    "def vocabulary_construction(dataset):\n",
    "    vocab = []\n",
    "    for string in dataset:\n",
    "        string_split = string.split()\n",
    "        for word in string_split:\n",
    "            if word not in vocab:\n",
    "                vocab.append(word)\n",
    "    return vocab\n",
    "# run given function to produce training vocabulary\n",
    "train_vocab = vocabulary_construction(clean_train_tweets)\n",
    "train_vocab_wsw = vocabulary_construction(clean_sw_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "oeFpqBMukUha"
   },
   "outputs": [],
   "source": [
    "num_train_tweets = len(clean_train_tweets)\n",
    "num_test_tweets = len(clean_train_tweets)\n",
    "\n",
    "def populate_bog(dataset, vocab):\n",
    "    vocab_size = len(vocab)\n",
    "    dataset_size = len(dataset)\n",
    "    bag_of_words = np.zeros((dataset_size, vocab_size), dtype='int')\n",
    "    for i in range(dataset_size):\n",
    "        current_tweet = dataset[i].split()\n",
    "        for word in current_tweet:\n",
    "            try:\n",
    "                position = vocab.index(word)\n",
    "#                 bag_of_words[i,position] += 1\n",
    "#                 print(bag_of_words[i][position])\n",
    "                bag_of_words[i][position] += 1\n",
    "#                 print(bag_of_words[i][position])\n",
    "            except:\n",
    "                pass\n",
    "    return bag_of_words\n",
    "\n",
    "# running to generate bag of words for test and training datasets\n",
    "train_bog = populate_bog(clean_train_tweets, train_vocab)\n",
    "test_bog = populate_bog(clean_test_tweets, train_vocab)\n",
    "\n",
    "train_bog_wsw = populate_bog(clean_sw_train, train_vocab_wsw)\n",
    "test_bog_wsw = populate_bog(clean_sw_test, train_vocab_wsw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g4z2MjAKB_xN"
   },
   "source": [
    "# Part 3: Logistic Regression from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "_oMyNpYnM-rt"
   },
   "outputs": [],
   "source": [
    "def sigmoid_function(x_vector, weight_vector):\n",
    "    z_value = np.dot(x_vector, weight_vector)\n",
    "#     z_value += weight_vector[0]\n",
    "    to_return = 1/(1+np.exp(-1*z_value))\n",
    "    return to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "j8X8qac-BvhZ"
   },
   "outputs": [],
   "source": [
    "# cross entropy function calls sigmoid function\n",
    "def cross_entropy_loss(label_set, input_dataset, weight_vector):\n",
    "    sum = 0\n",
    "    bias = weight_vector[0]\n",
    "    label_size = len(label_set)\n",
    "    for i in range(label_size):\n",
    "        x_vector = input_dataset[i]\n",
    "        y_label = label_set[i]\n",
    "        hypothesis_value = sigmoid_function(x_vector, weight_vector) # using first index onwards of the actual weight vector, as the 0th index has the bias value\n",
    "        current_value = y_label*np.log(hypothesis_value)+(1-y_label)*np.log(1-hypothesis_value)\n",
    "        sum += current_value\n",
    "    return -1*sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "2-IKlu9gmj9V"
   },
   "outputs": [],
   "source": [
    "# preparing dataset for logistic regression\n",
    "# making label vector for every case\n",
    "def label_preparation(label, dataset):\n",
    "    label_list = []\n",
    "    for datapoint in dataset:\n",
    "        if datapoint == label:\n",
    "            label_list.append(1)\n",
    "        else:\n",
    "            label_list.append(0)\n",
    "    return label_list\n",
    "\n",
    "neg_train_label_list = label_preparation('negative', train_sentiment)\n",
    "pos_train_label_list = label_preparation('positive', train_sentiment)\n",
    "neut_train_label_list = label_preparation('neutral', train_sentiment)\n",
    "\n",
    "neg_test_label_list = label_preparation('negative', test_sentiment)\n",
    "pos_test_label_list = label_preparation('positive', test_sentiment)\n",
    "neut_test_label_list = label_preparation('neutral', test_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "YkadrV8uP4C6"
   },
   "outputs": [],
   "source": [
    "# training using gradient descent\n",
    "def gradient_descent(initial_weights, dataset, label_vector, regular_param, epochs, lr):\n",
    "    # lr is learning rate\n",
    "    current_weights = initial_weights\n",
    "    for epoch in range(epochs):\n",
    "        sum = 0\n",
    "        bias = 0\n",
    "        for index, x_vector in enumerate(dataset):\n",
    "            expression = sigmoid_function(x_vector, current_weights)\n",
    "            label = label_vector[index]\n",
    "            expression -= label\n",
    "            bias_delta = expression\n",
    "            expression = expression*x_vector\n",
    "            sum += expression\n",
    "            bias += bias_delta\n",
    "        sum += 2*regular_param*current_weights[1:]\n",
    "        bias += 2*regular_param*current_weights[0]\n",
    "        current_weights[1:] = current_weights[1:] - lr*expression\n",
    "        current_weights[0] = current_weights[0] - lr*bias \n",
    "    return current_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "MNmI3xWneIyp"
   },
   "outputs": [],
   "source": [
    "def gradient_descent_vectorized(initial_weights, dataset, label_vector, regular_param, epochs, lr):\n",
    "    current_weights = initial_weights\n",
    "    for epoch in range(epochs):\n",
    "        expression_vector = sigmoid_function(dataset, current_weights)\n",
    "        expression_vector -= label_vector\n",
    "        bias = np.sum(expression_vector)\n",
    "        product = np.multiply(expression_vector, dataset.transpose())\n",
    "        sum = np.sum(product, axis=1)\n",
    "        delta_weights = sum + 2*regular_param*current_weights\n",
    "        # delta_bias = bias + 2*regular_param*current_weights[0]\n",
    "        # current_weights[1:] = current_weights[1:] - lr*delta_weights\n",
    "        # current_weights[0] - current_weights[0] - lr*delta_bias\n",
    "        current_weights = current_weights - lr/len(label_vector)*delta_weights\n",
    "    return current_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "KJecKMa6q23r"
   },
   "outputs": [],
   "source": [
    "def predict(dataset, weight_matrix):\n",
    "    predicted_labels = []\n",
    "    for x_vector in dataset:\n",
    "        predictions_list = []\n",
    "        for weight_vector in weight_matrix:\n",
    "            predictions_list.append(prediction_helper(x_vector, weight_vector))\n",
    "        predicted_label = predictions_list.index(max(predictions_list))\n",
    "        predicted_labels.append(predicted_label)\n",
    "    return predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Jza3yl95sdrw"
   },
   "outputs": [],
   "source": [
    "def prediction_helper(x_vector, weight_vector):\n",
    "    resulting_probability = np.dot(x_vector, weight_vector)\n",
    "#     resulting_probability += 1*weight_vector[0]\n",
    "    return resulting_probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j-9Er5RiuKlF"
   },
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "ksgXrOQb7CJh"
   },
   "outputs": [],
   "source": [
    "label_set = []\n",
    "label_set.append(neg_train_label_list)\n",
    "label_set.append(neut_train_label_list)\n",
    "label_set.append(pos_train_label_list)\n",
    "# neg,neut, pos = log_reg_training(train_bog, label_set, 1000, 0.01, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "GQoanvNmuOTL"
   },
   "outputs": [],
   "source": [
    "def log_reg_training(dataset, label_set, epochs, lr, regular_param):\n",
    "    initial_weights = np.zeros(len(dataset[0]))\n",
    "    negative_weights = gradient_descent_vectorized(initial_weights, dataset, label_set[0], regular_param, epochs, lr)\n",
    "    # print('done with negative')\n",
    "    neutral_weights = gradient_descent_vectorized(initial_weights, dataset, label_set[1], regular_param, epochs, lr)\n",
    "    # print('done with neutral')\n",
    "    positive_weights = gradient_descent_vectorized(initial_weights, dataset, label_set[2], regular_param, epochs, lr)\n",
    "    # print('done with positive')\n",
    "    weights_set = []\n",
    "    weights_set.append(negative_weights)\n",
    "    weights_set.append(neutral_weights)\n",
    "    weights_set.append(positive_weights)\n",
    "    return weights_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 337
    },
    "id": "dB6giQ2pML93",
    "outputId": "5aad3bb8-fcce-4540-ca94-1c2006b08e61"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (5856,7704) and (7703,) not aligned: 7704 (dim 1) != 7703 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-474aa65e1d08>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0me1000_set1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlog_reg_training\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_bog\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.0001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-19-f28c83c35e4a>\u001b[0m in \u001b[0;36mlog_reg_training\u001b[1;34m(dataset, label_set, epochs, lr, regular_param)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mlog_reg_training\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mregular_param\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0minitial_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mnegative_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgradient_descent_vectorized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minitial_weights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_set\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mregular_param\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;31m# print('done with negative')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mneutral_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgradient_descent_vectorized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minitial_weights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_set\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mregular_param\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-ee0fb38763e0>\u001b[0m in \u001b[0;36mgradient_descent_vectorized\u001b[1;34m(initial_weights, dataset, label_vector, regular_param, epochs, lr)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mcurrent_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minitial_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m         \u001b[0mexpression_vector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msigmoid_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         \u001b[0mexpression_vector\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mlabel_vector\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mbias\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexpression_vector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-050aa8767ac9>\u001b[0m in \u001b[0;36msigmoid_function\u001b[1;34m(x_vector, weight_vector)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msigmoid_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_vector\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight_vector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mz_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_vector\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight_vector\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mz_value\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mweight_vector\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mto_return\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mz_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mto_return\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (5856,7704) and (7703,) not aligned: 7704 (dim 1) != 7703 (dim 0)"
     ]
    }
   ],
   "source": [
    "e1000_set1 = log_reg_training(train_bog, label_set, 1000, 0.0001, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CAKKxmxs-nX2"
   },
   "outputs": [],
   "source": [
    "# computing all with 1000 epochs\n",
    "e1000_set1 = log_reg_training(train_bog, label_set, 1000, 0.0001, 1)\n",
    "e1000_set2 = log_reg_training(train_bog, label_set, 1000, 0.001, 1)\n",
    "e1000_set3 = log_reg_training(train_bog, label_set, 1000, 0.01, 1)\n",
    "e1000_set4 = log_reg_training(train_bog, label_set, 1000, 0.1, 1)\n",
    "e1000_set5 = log_reg_training(train_bog, label_set, 1000, 0.0001, 10)\n",
    "e1000_set6 = log_reg_training(train_bog, label_set, 1000, 0.001, 10)\n",
    "e1000_set7 = log_reg_training(train_bog, label_set, 1000, 0.01, 10)\n",
    "e1000_set8 = log_reg_training(train_bog, label_set, 1000, 0.1, 10)\n",
    "e1000_set9 = log_reg_training(train_bog, label_set, 1000, 0.0001, 0.1)\n",
    "e1000_set10 = log_reg_training(train_bog, label_set, 1000, 0.001, 0.1)\n",
    "e1000_set11 = log_reg_training(train_bog, label_set, 1000, 0.01, 0.1)\n",
    "e1000_set12 = log_reg_training(train_bog, label_set, 1000, 0.1, 0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mn_OgbxcJR34"
   },
   "outputs": [],
   "source": [
    "# computing all with 800 epochs\n",
    "e800_set1 = log_reg_training(train_bog, label_set, 800, 0.0001, 1)\n",
    "e800_set2 = log_reg_training(train_bog, label_set, 800, 0.001, 1)\n",
    "e800_set3 = log_reg_training(train_bog, label_set, 800, 0.01, 1)\n",
    "e800_set4 = log_reg_training(train_bog, label_set, 800, 0.1, 1)\n",
    "e800_set5 = log_reg_training(train_bog, label_set, 800, 0.0001, 10)\n",
    "e800_set6 = log_reg_training(train_bog, label_set, 800, 0.001, 10)\n",
    "e800_set7 = log_reg_training(train_bog, label_set, 800, 0.01, 10)\n",
    "e800_set8 = log_reg_training(train_bog, label_set, 800, 0.1, 10)\n",
    "e800_set9 = log_reg_training(train_bog, label_set, 800, 0.0001, 0.1)\n",
    "e800_set10 = log_reg_training(train_bog, label_set, 800, 0.001, 0.1)\n",
    "e800_set11 = log_reg_training(train_bog, label_set, 800, 0.01, 0.1)\n",
    "e800_set12 = log_reg_training(train_bog, label_set, 800, 0.1, 0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2zbA1A1nJ1-F"
   },
   "outputs": [],
   "source": [
    "# computing all with 500 epochs\n",
    "e500_set1 = log_reg_training(train_bog, label_set, 500, 0.0001, 1)\n",
    "e500_set2 = log_reg_training(train_bog, label_set, 500, 0.001, 1)\n",
    "e500_set3 = log_reg_training(train_bog, label_set, 500, 0.01, 1)\n",
    "e500_set4 = log_reg_training(train_bog, label_set, 500, 0.1, 1)\n",
    "e500_set5 = log_reg_training(train_bog, label_set, 500, 0.0001, 10)\n",
    "e500_set6 = log_reg_training(train_bog, label_set, 500, 0.001, 10)\n",
    "e500_set7 = log_reg_training(train_bog, label_set, 500, 0.01, 10)\n",
    "e500_set8 = log_reg_training(train_bog, label_set, 500, 0.1, 10)\n",
    "e500_set9 = log_reg_training(train_bog, label_set, 500, 0.0001, 0.1)\n",
    "e500_set10 = log_reg_training(train_bog, label_set, 500, 0.001, 0.1)\n",
    "e500_set11 = log_reg_training(train_bog, label_set, 500, 0.01, 0.1)\n",
    "e500_set12 = log_reg_training(train_bog, label_set, 500, 0.1, 0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "El6v-TbKK-IS"
   },
   "outputs": [],
   "source": [
    "# computing all with 200 epochs\n",
    "e200_set1 = log_reg_training(train_bog, label_set, 200, 0.0001, 1)\n",
    "e200_set2 = log_reg_training(train_bog, label_set, 200, 0.001, 1)\n",
    "e200_set3 = log_reg_training(train_bog, label_set, 200, 0.01, 1)\n",
    "e200_set4 = log_reg_training(train_bog, label_set, 200, 0.1, 1)\n",
    "e200_set5 = log_reg_training(train_bog, label_set, 200, 0.0001, 10)\n",
    "e200_set6 = log_reg_training(train_bog, label_set, 200, 0.001, 10)\n",
    "e200_set7 = log_reg_training(train_bog, label_set, 200, 0.01, 10)\n",
    "e200_set8 = log_reg_training(train_bog, label_set, 200, 0.1, 10)\n",
    "e200_set9 = log_reg_training(train_bog, label_set, 200, 0.0001, 0.1)\n",
    "e200_set10 = log_reg_training(train_bog, label_set, 200, 0.001, 0.1)\n",
    "e200_set11 = log_reg_training(train_bog, label_set, 200, 0.01, 0.1)\n",
    "e200_set12 = log_reg_training(train_bog, label_set, 200, 0.1, 0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "vywGBOXoLMrC"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-fedebd27a69a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# computing all with 100 epochs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0me100_set1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlog_reg_training\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_bog\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.0001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0me100_set2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlog_reg_training\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_bog\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0me100_set3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlog_reg_training\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_bog\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0me100_set4\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlog_reg_training\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_bog\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-f28c83c35e4a>\u001b[0m in \u001b[0;36mlog_reg_training\u001b[1;34m(dataset, label_set, epochs, lr, regular_param)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mlog_reg_training\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mregular_param\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0minitial_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mnegative_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgradient_descent_vectorized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minitial_weights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_set\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mregular_param\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;31m# print('done with negative')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mneutral_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgradient_descent_vectorized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minitial_weights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_set\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mregular_param\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-ee0fb38763e0>\u001b[0m in \u001b[0;36mgradient_descent_vectorized\u001b[1;34m(initial_weights, dataset, label_vector, regular_param, epochs, lr)\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mexpression_vector\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mlabel_vector\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mbias\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexpression_vector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mproduct\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexpression_vector\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0msum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproduct\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mdelta_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mregular_param\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mcurrent_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# computing all with 100 epochs\n",
    "e100_set1 = log_reg_training(train_bog, label_set, 100, 0.0001, 1)\n",
    "e100_set2 = log_reg_training(train_bog, label_set, 100, 0.001, 1)\n",
    "e100_set3 = log_reg_training(train_bog, label_set, 100, 0.01, 1)\n",
    "e100_set4 = log_reg_training(train_bog, label_set, 100, 0.1, 1)\n",
    "e100_set5 = log_reg_training(train_bog, label_set, 100, 0.0001, 10)\n",
    "e100_set6 = log_reg_training(train_bog, label_set, 100, 0.001, 10)\n",
    "e100_set7 = log_reg_training(train_bog, label_set, 100, 0.01, 10)\n",
    "e100_set8 = log_reg_training(train_bog, label_set, 100, 0.1, 10)\n",
    "e100_set9 = log_reg_training(train_bog, label_set, 100, 0.0001, 0.1)\n",
    "e100_set10 = log_reg_training(train_bog, label_set, 100, 0.001, 0.1)\n",
    "e100_set11 = log_reg_training(train_bog, label_set, 100, 0.01, 0.1)\n",
    "e100_set12 = log_reg_training(train_bog, label_set, 100, 0.1, 0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "OIXMiaJiEfxb"
   },
   "outputs": [],
   "source": [
    "# random training\n",
    "e200_set2 = log_reg_training(train_bog, label_set, 200, 0.001, 3)\n",
    "e200_set22 = log_reg_training(train_bog, label_set, 200, 0.001, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vvMHNFGiL3C7"
   },
   "outputs": [],
   "source": [
    "e500_set1 = log_reg_training(train_bog, label_set, 500, 0.0001, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "e100_set1 = log_reg_training(train_bog, label_set, 100, 0.001, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QeKH8IgBIvFC",
    "outputId": "919c6923-b018-4f5d-e356-4205ad906803",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions_set1 = predict(test_bog, e100_set1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MlGYuO_hLuo5"
   },
   "source": [
    "## Evaluation of Algorithm\n",
    "## (Functions to be used for evaluation of Part 5 as well)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "RaMI8pXE_en3"
   },
   "outputs": [],
   "source": [
    "def evaluation_function(predicted_labels, actual_labels):\n",
    "    # construct confusion matrix\n",
    "    # report accuracy\n",
    "    # report f1 score\n",
    "    num_predictions = len(predicted_labels)\n",
    "    correct_count = 0\n",
    "    # constructing confusion matrix\n",
    "    confusion_matrix = np.zeros((3,3))\n",
    "    # 0 is negative, 1 is neutral, 2 is positive\n",
    "    for i in range(num_predictions):\n",
    "        if predicted_labels[i] == actual_labels[i]:\n",
    "            correct_count += 1\n",
    "        confusion_matrix[predicted_labels[i]][actual_labels[i]] += 1\n",
    "#         confusion_matrix[predicted_labels[i], actual_labels[i]] += 1\n",
    "    accuracy = correct_count/num_predictions\n",
    "\n",
    "    # computing macro averages\n",
    "    # computing recall\n",
    "    recall_0 = confusion_matrix[0,0]/np.sum(confusion_matrix[:,0])\n",
    "    recall_1 = confusion_matrix[1,1]/np.sum(confusion_matrix[:,1])\n",
    "    recall_2 = confusion_matrix[2,2]/np.sum(confusion_matrix[:,2])\n",
    "\n",
    "    # computing precision\n",
    "    try:\n",
    "        precision_0 = confusion_matrix[0,0]/np.sum(confusion_matrix[0])\n",
    "    except:\n",
    "        precision_0 = 0\n",
    "    try:\n",
    "        precision_1 = confusion_matrix[1,1]/np.sum(confusion_matrix[1])\n",
    "    except:\n",
    "        precision_1 = 0\n",
    "    try:\n",
    "        precision_2 = confusion_matrix[2,2]/np.sum(confusion_matrix[2])\n",
    "    except: precision_2 = 0\n",
    "\n",
    "    macro_avg_recall = (recall_0 + recall_1 + recall_2)/3\n",
    "    macro_avg_precision = (precision_0 + precision_1 + precision_2)/3\n",
    "\n",
    "    macro_f1_score = 2*macro_avg_recall*macro_avg_precision/(macro_avg_recall+macro_avg_precision)\n",
    "    print('macro f1', macro_f1_score)\n",
    "    print('Confusion Matrix')\n",
    "    print(confusion_matrix)\n",
    "    return accuracy, confusion_matrix, macro_f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mTmk4q9DFKWW",
    "outputId": "76903c89-032d-4f44-c19a-5de9eb73b5c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro f1 0.4820779505250031\n",
      "Confusion Matrix\n",
      "[[923. 296. 223.]\n",
      " [  0.   3.   1.]\n",
      " [  1.   2.  15.]]\n"
     ]
    }
   ],
   "source": [
    "# predictions_set1 = predict(test_bog, e1000_set1)\n",
    "# print(list(set(e1000_set1[0])))\n",
    "# print(list(set(e1000_set1[1])))\n",
    "# print(list(set(e1000_set1[2])))\n",
    "_,_, _ = evaluation_function(predictions_set1, encoded_test_labels)\n",
    "# print(predictions_set1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "CH5jcreHEjeG"
   },
   "outputs": [],
   "source": [
    "# encoding of the actual labels\n",
    "encoded_train_labels = []\n",
    "for label in train_sentiment:\n",
    "    if label == 'positive':\n",
    "        encoded_train_labels.append(2)\n",
    "    elif label == 'neutral':\n",
    "        encoded_train_labels.append(1)\n",
    "    elif label == 'negative':\n",
    "        encoded_train_labels.append(0)\n",
    "\n",
    "encoded_test_labels = []\n",
    "for label in test_sentiment:\n",
    "    if label == 'positive':\n",
    "        encoded_test_labels.append(2)\n",
    "    elif label == 'neutral':\n",
    "        encoded_test_labels.append(1)\n",
    "    elif label == 'negative':\n",
    "        encoded_test_labels.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "-Rfgot46FJf_"
   },
   "outputs": [],
   "source": [
    "def compute_loss(dataset, label_set, weight_vector):\n",
    "    loss = 0\n",
    "    for i in range(3):\n",
    "        loss += cross_entropy_loss(label_set[i], dataset, weight_vector[i])\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "asO5rg-vL5Pi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9799.821360453887\n",
      "8984.374609249742\n",
      "8838.66759931274\n",
      "17697.97377454303\n",
      "9799.841363549089\n"
     ]
    }
   ],
   "source": [
    "loss1 = compute_loss(train_bog, label_set, e100_set1)\n",
    "print(loss1)\n",
    "loss2 = compute_loss(train_bog, label_set, e100_set2)\n",
    "print(loss2)\n",
    "loss3 = compute_loss(train_bog, label_set, e100_set3)\n",
    "print(loss3)\n",
    "loss4 = compute_loss(train_bog, label_set, e100_set4)\n",
    "print(loss4)\n",
    "loss5 = compute_loss(train_bog, label_set, e100_set5)\n",
    "print(loss5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ly_FJrrONBMK"
   },
   "source": [
    "# Part 4: Logistic Regression using Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-00uG-qrNHri",
    "outputId": "0a77ed80-a74a-43ee-d6a8-ff6955d6f7b5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=1000, multi_class='ovr')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_reg = LogisticRegression(multi_class='ovr', max_iter=1000)\n",
    "logistic_reg.fit(train_bog, train_sentiment) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zPcPjyqaPysP"
   },
   "source": [
    "The model is trained using the training bag of words as the input data, along with the labels passed as 'negative', 'positive', or 'neutral'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hDSOg3DxNB-P",
    "outputId": "a7bd262e-16ba-4539-ae34-3e7efac63ec3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.90      0.88       924\n",
      "     neutral       0.60      0.59      0.60       301\n",
      "    positive       0.80      0.62      0.70       239\n",
      "\n",
      "    accuracy                           0.79      1464\n",
      "   macro avg       0.75      0.70      0.72      1464\n",
      "weighted avg       0.79      0.79      0.79      1464\n",
      "\n",
      "Confusion Matrix:\n",
      "1. Negative, 2. Neutral 3. Positive - this is applicable for both rows and columns\n",
      "[[835  74  15]\n",
      " [102 178  21]\n",
      " [ 47  44 148]]\n"
     ]
    }
   ],
   "source": [
    "predictions = logistic_reg.predict(test_bog)\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "report = classification_report(test_sentiment, predictions)\n",
    "con_mat = confusion_matrix(test_sentiment, predictions)\n",
    "print(report)\n",
    "print('Confusion Matrix:')\n",
    "print('1. Negative, 2. Neutral 3. Positive - this is applicable for both rows and columns')\n",
    "print(con_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5GYkEtX8PDyN"
   },
   "source": [
    "The confusion matrix has been plotted above. The accuracy comes out to be roughly 0.79. The macro average of the f1 score is 0.72, while the weighted average of the f1-score is 0.79. As can be seen, there is much more data available for the negative sentiment as compared to neutral and positive tweets. Furthermore, the f1 score is generally high for negative tweets as well, showing that the classifier performs generally better in that case.\n",
    "\n",
    "Summary:\n",
    "<br>\n",
    "Accuracy: 0.79\n",
    "<br>Macro Average F1-score: 0.72\n",
    "<br>Weighted Average F1-score: 0.79"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gUlfrEuTP_NO"
   },
   "source": [
    "# Part 5: Implementation of Naive Bayes Classifier from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bFZt7AFAQF5X"
   },
   "outputs": [],
   "source": [
    "# try with removing stop words and without removing stop words\n",
    "# prepare the two bag of words\n",
    "# already have one without the stop words\n",
    "\n",
    "# need to apply laplace smoothing to the bag of words\n",
    "# calculate prior probabilities\n",
    "\n",
    "# training bag of words is train_bog\n",
    "# testing bag of words is test_bog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making one whole bag of words for training\n",
    "NB_bow_neg = np.zeros((len(train_vocab)))\n",
    "NB_bow_pos = np.zeros((len(train_vocab)))\n",
    "NB_bow_neu = np.zeros((len(train_vocab)))\n",
    "for tweet_index,tweet in enumerate(clean_train_tweets):\n",
    "    tweet = tweet.split()\n",
    "    for word in tweet:\n",
    "        index = train_vocab.index(word)\n",
    "        if train_sentiment[tweet_index] == 'negative':\n",
    "            NB_bow_neg[index] += 1\n",
    "        elif train_sentiment[tweet_index] == 'positive':\n",
    "            NB_bow_pos[index] += 1\n",
    "        elif train_sentiment[tweet_index] == 'neutral':\n",
    "            NB_bow_neu[index] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_total = np.sum(NB_bow_neg)\n",
    "pos_total = np.sum(NB_bow_pos)\n",
    "neu_total = np.sum(NB_bow_neu)\n",
    "\n",
    "# add 1 smoothing\n",
    "total_add = len(train_vocab)\n",
    "neg_total += total_add\n",
    "pos_total += total_add\n",
    "neu_total += total_add\n",
    "\n",
    "NB_bow_neg += 1\n",
    "NB_bow_pos += 1\n",
    "NB_bow_neu += 1\n",
    "\n",
    "# calculating likelihood for each word\n",
    "neg_likelihood = NB_bow_neg / neg_total\n",
    "pos_likelihood = NB_bow_pos / pos_total\n",
    "neu_likelihood = NB_bow_neu / neu_total\n",
    "\n",
    "likelihood_matrix = []\n",
    "likelihood_matrix.append(neg_likelihood)\n",
    "likelihood_matrix.append(pos_likelihood)\n",
    "likelihood_matrix.append(neu_likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0010480747322852587\n"
     ]
    }
   ],
   "source": [
    "print(likelihood_matrix[0][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes_predictor(input_mat, vocab, likelihood_mat):\n",
    "    predictions = []\n",
    "    for tweet in input_mat:\n",
    "        tweet = tweet.split()\n",
    "        probs = [0,0,0]\n",
    "        for word in tweet:\n",
    "            try:\n",
    "                word_index = vocab.index(word)\n",
    "                probs[0] += likelihood_mat[0][word_index]\n",
    "                probs[1] += likelihood_mat[2][word_index]\n",
    "                probs[2] += likelihood_mat[1][word_index]\n",
    "            except:\n",
    "                pass\n",
    "        prediction = probs.index(max(probs))\n",
    "        predictions.append(prediction)\n",
    "    predictions = np.array(predictions)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro f1 0.5203603719046199\n",
      "Confusion Matrix\n",
      "[[923.   0.   1.]\n",
      " [285.   7.   9.]\n",
      " [201.   1.  37.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.6605191256830601,\n",
       " array([[923.,   0.,   1.],\n",
       "        [285.,   7.,   9.],\n",
       "        [201.,   1.,  37.]]),\n",
       " 0.5203603719046199)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_NB = naive_bayes_predictor(clean_test_tweets, train_vocab, likelihood_matrix)\n",
    "evaluation_function(encoded_test_labels, predictions_NB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bn4UoH1X_HNL"
   },
   "source": [
    "# Part 6: Scikit-Learn implementation of Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m2k6KBf3_gc3",
    "outputId": "4ccca069-c19f-40af-dc4d-14f80d9c3183"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NB_skl_model = MultinomialNB()\n",
    "NB_skl_model.fit(train_bog, train_sentiment, sample_weight=None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MdqHfI7PA_mO",
    "outputId": "b6fa8049-ebd9-47d1-fe3b-456770a68e5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.76      0.97      0.85       924\n",
      "     neutral       0.71      0.32      0.44       301\n",
      "    positive       0.81      0.49      0.61       239\n",
      "\n",
      "    accuracy                           0.76      1464\n",
      "   macro avg       0.76      0.59      0.64      1464\n",
      "weighted avg       0.76      0.76      0.73      1464\n",
      "\n",
      "Confusion Matrix\n",
      "[[899  19   6]\n",
      " [182  97  22]\n",
      " [102  20 117]]\n"
     ]
    }
   ],
   "source": [
    "NB_predictions = NB_skl_model.predict(test_bog)\n",
    "nb_report = classification_report(test_sentiment, NB_predictions)\n",
    "nb_con_mat = confusion_matrix(test_sentiment, NB_predictions)\n",
    "\n",
    "print(nb_report)\n",
    "print('Confusion Matrix')\n",
    "print(nb_con_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gNpmcTRgC-uy"
   },
   "source": [
    "The results are relatively similar to the Logistic Regression model.\n",
    "\n",
    "We have more data for negative tweets, and hence the prediction is generally stronger for them (as can be seen with the f1-score etc). However, the overall accuracy is generally about as good as it was for the logistic regression model. What we will also investigate is the effect of removing stop words. We will now run the model with the stop words included and evaluate performance. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "0Tc_NoMaDn-5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.76      0.97      0.85       924\n",
      "     neutral       0.71      0.32      0.44       301\n",
      "    positive       0.81      0.49      0.61       239\n",
      "\n",
      "    accuracy                           0.76      1464\n",
      "   macro avg       0.76      0.59      0.64      1464\n",
      "weighted avg       0.76      0.76      0.73      1464\n",
      "\n",
      "Confusion Matrix\n",
      "[[899  19   6]\n",
      " [182  97  22]\n",
      " [102  20 117]]\n"
     ]
    }
   ],
   "source": [
    "NB_skl_model_wsw = MultinomialNB()\n",
    "NB_skl_model_wsw.fit(train_bog_wsw, train_sentiment, sample_weight=None) \n",
    "\n",
    "NB_predictions_wsw = NB_skl_model.predict(test_bog_wsw)\n",
    "nb_report_wsw = classification_report(test_sentiment, NB_predictions_wsw)\n",
    "nb_con_mat_wsw = confusion_matrix(test_sentiment, NB_predictions_wsw)\n",
    "\n",
    "print(nb_report_wsw)\n",
    "print('Confusion Matrix')\n",
    "print(nb_con_mat_wsw)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen comparing the two confusion matrices, there is no difference in the prediction, regardless of whether or not we include the stop words. This can be seen as a strength of the Naive Bayes classifier, as it would not require the removal of stop words. To further accentuate this claim, we will also test the Logistic Regression model with stop words and see its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.90      0.88       924\n",
      "     neutral       0.60      0.59      0.60       301\n",
      "    positive       0.80      0.62      0.70       239\n",
      "\n",
      "    accuracy                           0.79      1464\n",
      "   macro avg       0.75      0.70      0.72      1464\n",
      "weighted avg       0.79      0.79      0.79      1464\n",
      "\n",
      "Confusion Matrix:\n",
      "1. Negative, 2. Neutral 3. Positive - this is applicable for both rows and columns\n",
      "[[835  74  15]\n",
      " [102 178  21]\n",
      " [ 47  44 148]]\n"
     ]
    }
   ],
   "source": [
    "logistic_reg = LogisticRegression(multi_class='ovr', max_iter=1000)\n",
    "logistic_reg.fit(train_bog_wsw, train_sentiment) \n",
    "\n",
    "predictions = logistic_reg.predict(test_bog_wsw)\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "report = classification_report(test_sentiment, predictions)\n",
    "con_mat = confusion_matrix(test_sentiment, predictions)\n",
    "print(report)\n",
    "print('Confusion Matrix:')\n",
    "print('1. Negative, 2. Neutral 3. Positive - this is applicable for both rows and columns')\n",
    "print(con_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inclusion of stop words does not seem to have an effect on the Logistic Regression classifier either."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Evaluation+ ML Assignment 4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
